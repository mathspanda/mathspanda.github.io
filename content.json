{"meta":{"title":"mathspandaのNotes","subtitle":null,"description":"聪明人才知道自己是笨蛋","author":"mathspanda","url":"http://yoursite.com"},"pages":[{"title":"分类","date":"2018-06-23T14:01:15.000Z","updated":"2018-06-23T14:52:42.000Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"关于我","date":"2018-06-23T14:51:26.000Z","updated":"2019-02-11T15:40:29.652Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"毕业于上海某 TOP3 大学的一名小硕士，热衷于分布式系统、容器等技术。 目前专注于 DB/MQ On Kubernetes 以及 Istio。"},{"title":"我的阅读书单","date":"2018-03-09T09:28:30.000Z","updated":"2019-07-10T15:13:41.040Z","comments":true,"path":"readings/index.html","permalink":"http://yoursite.com/readings/index.html","excerpt":"","text":"2019年 Redis开发与运维 Redis设计与实现 深入浅出Istio：Service Mesh快速入门与实践 Apache Kafka实战 深入RabbitMQ Kubernetes进阶实战 2018年。。。 2017年 剑指Offer：名企面试官精讲典型编程题（第2版） 自己动手写Docker Docker生产环境实践指南 Kubernetes权威指南：从Docker到Kubernetes实践全接触（第2版） Docker实战 图解TCP/IP（第5版） 2016年 大型网站技术架构：核心原理与案例分析 深入理解Java虚拟机（第2版）：JVM高级特性与最佳实践 微服务设计 大规模分布式存储系统：原理解析与架构实战"},{"title":"标签","date":"2018-06-23T14:01:21.000Z","updated":"2018-06-23T14:51:45.000Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Jaeger on Kubernetes 部署总结","slug":"jaeger-deploy","date":"2018-09-19T09:58:02.000Z","updated":"2018-11-24T15:51:33.281Z","comments":true,"path":"2018/09/19/jaeger-deploy/","link":"","permalink":"http://yoursite.com/2018/09/19/jaeger-deploy/","excerpt":"根据 Jaeger 1.6 官方的部署文档，部署一个完整的 Jaeger 分布式链路追踪系统需要部署以下组件：","text":"根据 Jaeger 1.6 官方的部署文档，部署一个完整的 Jaeger 分布式链路追踪系统需要部署以下组件： storage backend: 存储 trace 数据，支持 elasticsearch / cassandra / memory（仅支持 all-in-one 部署）以及 kafka（不完善，最新 1.6 版本仅支持写 trace，查询时需对接另一种 storage backend），通过环境变量 SPAN_STORAGE_TYPE 指定。 jaeger-query: 从 storage 中查询 trace 及前端 ui 展示。 jaeger-collector: 接收发送自 jaeger-agent 的 trace 数据（或者可直接发送 zipkin spans 至 collector），验证、索引、转换及存储 trace 数据。 jaeger-agent: client 与 collector 中间的代理层，监听发送过来的 spans 数据并批量发送至 collector。 上述四种组件可直接通过 all-in-one 的方式部署（此时 storage backend 为 memory），由于存在 trace 数据丢失、组件单点故障等缺陷，仅建议开发环境下搭建使用。 当前的 Istio Helm 套件中包含了开启 jaeger 的开关（默认不开启），通过 helm 部署 istio 时加上 --set tracing.enabled=true 选项即可开启，在 istio-system namespace 下会创建一个名为 istio-tracing 的 deployment，以 all-in-one 方式部署。 推荐的部署方式是将各 jaeger 组件分散部署： query 与 collector 通过 deployment 部署（两者均为无状态服务） agent 通过 daemonset 或者 sidecar container 部署 单独建立高可靠可用的 elasticsearch 或者 cassandra 集群 jaeger-agent daemonset vs sidecar pod daemonset: 同一 node 上的所有 pod 将 trace 数据发往同一个 jaeger-agent，适用于单租户环境，同一 node 上的所有 pod 被同等对待。此部署方式的内存消耗较小，然而每一个 agent 可能需要照顾到几百个 pod。 sidecar container: agent 作为 container 与 application pod 一同部署，运行在应用程序级别。不同的应用可发送 trace 数据至不同的 collector，适用于多租户环境。 前者适用于私有云，平台上运行的应用可信任；后者适用于公有云或者多租户需求，同时会带来一定的内存消耗（每个 agent 消耗大约 20MB 内存）。 elasticsearch vs cassandra elasticsearch 查询性能优于 cassandra，cassandra 写性能相对更优 elasticsearch 的定位是 search engine，而不是 persistent store，偶尔会丢失部分 writes，对大量 trace 数据而言可接受（这点同 logs 与 metrics，接受部分数据丢失） 此处选择 es 集群（对 es 运维比较熟悉），es 从 jaeger 0.6.0 时即开始支持，且不需要作手动初始化（cassandra 需手动创建 schema）。 jaeger-kubernetes vs jaeger-helm-charthelm charts 中包含了一个 jaeger helm chart，尚处于 incubator 阶段，到当前为止已有 4 个月未作任何更新，且版本较老，因此暂时放弃。 jaeger-kubernetes 项目中包含了将 jaeger 组件分散部署的 yml 文件，通过以下命令可创建一个分散部署的 jaeger：s123456# 创建包含 es / jaeger 配置文件的 ConfigMap$ kubectl create -f https://raw.githubusercontent.com/jaegertracing/jaeger-kubernetes/master/production-elasticsearch/configmap.yml # 创建单点 elasticsearch 集群$ kubectl create -f https://raw.githubusercontent.com/jaegertracing/jaeger-kubernetes/master/production-elasticsearch/elasticsearch.yml # 创建组件分散部署的 jaeger（agent 通过 daemonset 部署）$ kubectl create -f https://raw.githubusercontent.com/jaegertracing/jaeger-kubernetes/master/jaeger-production-template.yml 通过上述方式部署的 es 集群采用的是 emptyDir volume，且以单点方式部署，同样不适用于生产环境。因此需要在使用上述 configmap.yml 与 jaeger-production-template.yml 部署 jaeger 之前，单独建立一个高可靠可用的 es 集群，通过 ceph/glusterfs 等存储保障 trace 数据不丢失。 以上面的 yml 文件为基础，可以自行编写一套 helm chart，扩展各种 spec 配置，如 resource 和 affinity 等。 部署步骤创建 elasticsearch 集群假设当前 kubernetes 集群上包含 ceph storageclass，借助 helm charts 中的 elasticsearch helm chart：12$ helm template elasticsearch --name jaeger-es --set cluster.name=tracing-es --set master.persistence.storageClass=ceph --set data.persistence.storageClass=ceph &gt; jaeger-es.yaml$ kubectl create -f jaeger-es.yaml -n $&#123;namespace&#125; 123456789101112131415161718root@lin-k8s-dev1:~# kubectl -n $&#123;namespace&#125; get po | grep jaeger-esjaeger-es-elasticsearch-client-54666b4cf5-7lpbg 1/1 Running 2 1djaeger-es-elasticsearch-client-54666b4cf5-scw79 1/1 Running 2 1djaeger-es-elasticsearch-data-0 1/1 Running 0 1djaeger-es-elasticsearch-data-1 1/1 Running 0 1djaeger-es-elasticsearch-master-0 1/1 Running 0 1djaeger-es-elasticsearch-master-1 1/1 Running 0 1djaeger-es-elasticsearch-master-2 1/1 Running 0 1droot@lin-k8s-dev1:~# kubectl -n $&#123;namespace&#125; get sts | grep jaeger-esjaeger-es-elasticsearch-data 2 2 1djaeger-es-elasticsearch-master 3 3 1droot@lin-k8s-dev1:~# kubectl -n $&#123;namespace&#125; get deploy | grep jaeger-esjaeger-es-elasticsearch-client 2 2 2 2 1droot@lin-k8s-dev1:~# kubectl -n $&#123;namespace&#125; get cm | grep jaeger-esjaeger-es-elasticsearch 4 1droot@lin-k8s-dev1:~# kubectl -n $&#123;namespace&#125; get svc | grep jaeger-esjaeger-es-elasticsearch-client ClusterIP 10.109.173.100 &lt;none&gt; 9200/TCP 1djaeger-es-elasticsearch-discovery ClusterIP None &lt;none&gt; 9300/TCP 默认情况下会创建 es master statefulset (replica=3)、data statefulset (replica=2) 以及 client deployment (replica=2)。 es master 负责 es 集群的元数据维护及节点状态管理，data 负责真实 index 数据的查询及存储，client 负责分散负载。 可通过 service jaeger-es-elasticsearch-client（9200端口）与 es 集群进行交互：curl http://jaeger-es-elasticsearch-client:9200/_cat/health?v 具体的 es 组件配置及资源配置需根据环境作针对调整。 创建 jaeger123456git clone https://github.com/jaegertracing/jaeger-kubernetescd jaeger-kubernetes# 修改 production-elasticsearch/configmap.yml 中的两处 es.server-urls 为 http://jaeger-es-elasticsearch-client.$&#123;namespace&#125;:9200kubectl -n $&#123;namespace&#125; create -f production-elasticsearch/configmap.yml# 调整 jaeger-production-template.yml 中 query 及 collector deployments 的 replicas 及 resources 等字段kubectl -n $&#123;namespace&#125; create -f jaeger-production-template.yml 123456789101112131415root@lin-k8s-dev1:~# kubectl -n $&#123;namespace&#125; get po | grep -v es | grep jaegerjaeger-agent-wtmn8 1/1 Running 0 1djaeger-collector-6b58d6c587-fs77b 1/1 Running 0 1djaeger-query-765c897dc4-p8nbh 1/1 Running 0 1droot@lin-k8s-dev1:~# kubectl -n $&#123;namespace&#125; get deploy | grep -v es | grep jaegerjaeger-collector 1 1 1 1 1djaeger-query 1 1 1 1 1droot@lin-k8s-dev1:~# kubectl -n $&#123;namespace&#125; get ds | grep -v es | grep jaegerjaeger-agent 1 1 1 1 1 &lt;none&gt; 1droot@lin-k8s-dev1:~# kubectl -n $&#123;namespace&#125; get cm | grep -v es | grep jaegerjaeger-configuration 4 2droot@lin-k8s-dev1:~# kubectl -n $&#123;namespace&#125; get svc | grep -v es | grep 'jaeger\\|zipkin'jaeger-collector ClusterIP 10.103.203.16 &lt;none&gt; 14267/TCP,14268/TCP,9411/TCP 1djaeger-query LoadBalancer 10.107.228.198 &lt;pending&gt; 80:31150/TCP 1dzipkin ClusterIP 10.109.123.192 &lt;none&gt; 9411/TCP 1d zipkin service 对应 jaeger-collector 的 9411 端口，zipkin spans 可直接通过该端口发送，无需经历 agent 代理。 与 istio 集成应用程序 pod 中被注入的 sidecar container envoy 默认配置是将 spans 数据通过 zipkin:9411 发送给 jaeger-collector，而jaeger-production-template.yml 中已包含端口号是 9411 的 zipkin service，因此与 istio 集成的话暂时不需要做什么。 值得注意的是，如果 jaeger 仅仅是部署给 istio 使用，并且 HTTP 应用服务之间直接通过 headers forward (ZipkinB3HTTPHeader) 的方式传递 trace 信息，可以不用部署 jaeger-agent 组件，spans 数据会直接发送至 jaeger-collector。 为了 trace 每个 request 的信息，需要将 istio chart values.yml 中的 enableTracing 开启（默认是开启的），这样才会打开 envoy.http_connection_manager 的 tracing 功能。 上述部署过程可直接形成一套 yaml，通过 apply -f 自动化部署，jaeger-query 与 jaeger-collector pod 初期由于连接不上 es 会频繁 CrashLoopBack 重启，待 es 集群启动完毕后，jaeger 组件也会进入 running 状态。 监控监控 elasticsearch 集群可直接采用开源项目 elasticsearch_exporter，helm charts 中包含了相应的部署 chart。 jaeger 各组件的 metrics 可通过相应的 HTTP 端口 得到。根据 jaeger 的 metrics，可以对组件的一些配置进行调整（edit configmap）。以 jaeger-collector 暴露的 spans_dropped 指标为例，往往是由于内部的 queue overflow 导致，可以相应调大 collector 的参数 collector.queue-size（默认为 2000），并依次重启 jaeger-collector pod。","categories":[{"name":"Tracing","slug":"Tracing","permalink":"http://yoursite.com/categories/Tracing/"}],"tags":[{"name":"jaeger","slug":"jaeger","permalink":"http://yoursite.com/tags/jaeger/"},{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/tags/kubernetes/"},{"name":"tracing","slug":"tracing","permalink":"http://yoursite.com/tags/tracing/"}]},{"title":"RabbitMQ 之 Flow Control 机制","slug":"rabbitmq-flow-control","date":"2018-06-24T09:13:20.000Z","updated":"2018-11-24T15:34:49.210Z","comments":true,"path":"2018/06/24/rabbitmq-flow-control/","link":"","permalink":"http://yoursite.com/2018/06/24/rabbitmq-flow-control/","excerpt":"最近尝试在将 RabbitMQ 搬上 Kubernetes 运行，因为有前车之鉴，担心 OOMKilled 的问题，特意调研一下 RabbitMQ 的流控机制。 当 RabbitMQ 消息消费速率小于发布速率，抑或是系统本身的资源就不太够时，为避免机器资源饱和，RabbitMQ 会阻断 publishers 进行流控。流控无需相关参数控制开启，这是 RabbitMQ 自身的默认行为。","text":"最近尝试在将 RabbitMQ 搬上 Kubernetes 运行，因为有前车之鉴，担心 OOMKilled 的问题，特意调研一下 RabbitMQ 的流控机制。 当 RabbitMQ 消息消费速率小于发布速率，抑或是系统本身的资源就不太够时，为避免机器资源饱和，RabbitMQ 会阻断 publishers 进行流控。流控无需相关参数控制开启，这是 RabbitMQ 自身的默认行为。 Memory-Based Flow Control如同字面上的意思，对内存设置阈值，超出阈值后阻断发布连接，相关参数列举如下： total_memory_available_override_value系统内存值。默认情况下 RabbitMQ 启动时会推断该值的大小，在某些特殊环境下推断得到的值不大准确，比如在容器内部可能得到的是宿主机的内存值，需手动指定。 vm_memory_high_watermark触发流控的内存阈值。默认为 0.4，意味着如果系统内存值为 1GB，则 RabbitMQ Server 在使用了 400MB 左右的内存时会阻断 pulish 连接。参数值的设置有相对与绝对两种：12vm_memory_high_watermark.relative = 0.6 vm_memory_high_watermark.absolute = 2GB 设置完成后，可在日志中找到相关的内存限制信息：12# vm_memory_high_watermark.relative = 0.4, total_memory_available_override_value = 700MBMemory high watermark set to 280 MiB (293601280 bytes) of 700 MiB (734003200 bytes) total 也可以在rabbitmqctl status的输出中找到：1234... &#123;vm_memory_high_watermark,0.4&#125;, &#123;vm_memory_limit,293601280&#125;,... 内存使用超过 40% 时，发出内存 alarm，阻塞连接；随着消息被消费或者被 page 到磁盘后，内存消耗降低，alarm 被撤销，publish 继续。 rabbitmqctl set_vm_memory_high_watermark 0 可禁止一切 publish 操作。 默认设置为 0.4 是由于 Erlang 的垃圾回收机制在最坏情况下会导致内存消耗翻倍，即 80%。 参数值的设置并不能完全限制住 RabbitMQ 的内存阈值。 官方文档对生产环境下参数值设置的建议： RabbitMQ 所在的机器最好时刻拥有 128MB 的空闲内存。 vm_memory_high_watermark 的推荐设置范围是 0.40 ~ 0.66。 不建议大于 0.7，以防大 paging 导致 OS 性能的急剧下降。 vm_memory_high_watermark_paging_ratio相对 vm_memory_high_watermark 触发消息 paging 至磁盘的内存比值。默认为 0.5，如果 vm_memory_high_watermark 设置为 0.4，系统内存值为 1GB，则 RabbitMQ 内存使用超过 0.5&times;0.4&times;1GB=200MB 时开始 paging。 当然，除了上述内存流控参数之外，还存在 disk_free_limit 等磁盘流控参数，具体说明可见官方文档。 Per-Connection Flow Control针对消息消费速率不及发布速率这种情况，应用实现方应该更加关注应用逻辑，而不是 RabbitMQ。然而 RabbitMQ 本身应当提供一种流控机制，防止由于出现类似情况而导致 OutOfDisk 或者 OOMKilled。除了之前提到的对内存和磁盘设置阈值之外，RabbitMQ 还默认提供了一种基于 credit flow 的流控机制，面向每一个连接进行流控，以确保 RabbitMQ 本身的稳定性。 RabbitMQ 由 Erlang 实现，消息从发布到最终被消费，会在 RabbitMQ 内部的多个进程间进行转发，大致的转发路径如下图： rabbit_reader 接收 client 连接，解析 AMQP 帧；rabbit_channel 处理 AMQP 方法，路由消息；rabbit_amqqueue_process 作为队列进程，负责实现队列本身的所有逻辑，该进程数量一般与队列数目相同；rabbit_msg_store 负责消息持久化；rabbit_writer 向 client 返回数据。 作为 Erlang 进程，每个进程均拥有自己的 mailbox，进程间通过消息进行通信。mailbox 大小默认不作设置，如果某进程由于负载较高来不及处理消息，则 mailbox 开始堆积消息，最终可能会内存溢出进而进程崩溃。RabbitMQ 实现了一种基于 credit flow 的流控机制，当 mailbox 中的消息堆积到一定程度时会阻塞上游进程，不得接收新消息，一级级向上阻塞，最终会导致源头负责接收网络数据包的进程阻塞，进而阻塞发布。 每个消息处理进程拥有一个 credit 元组 {InitialCredit, MoreCreditAfter}，如果不调用 credit_flow API 调整值，默认为 {200, 50}。以进程 A 向进程 B 发送消息为例，A 的初始 credit 为 InitialCredit，A 每向 B 发送一条消息， A 的 credit 减 1，直到 credit 为 0 时 A 进程被阻塞；B 每收到 A 的 MoreCreditAfter 条消息后，会向 A 发送消息，授予 A MoreCreditAfter 个 credit。A 的 credit 大于 0 后恢复，继续向 B 发送消息。这些 credit 数量被记录在 Erlang 进程的进程字典（Process Dictionary）中，相关字典键如下：1234&#123;credit_to, pid()&#125;&#123;credit_from, pid()&#125;&#123;credit_deferred&#125;&#123;credit_blocked&#125; A 中用以记录 B 授予 credit 数量的条目以{credit_from, From}为键，From 为进程 B 的 Pid，如果{credit_from, From}对应的值为 0，则 A 被阻塞，记录在以{credit_blocked}为键的条目中。当然，A 可能会向多个进程发送消息，比方说进程 C，这时候会同时存在以{credit_from, B}以及{credit_from, C}为键的条目；A 也可能会同时被进程 B 和 C 阻塞，阻塞进程的键值对条目大致如 {credit_blocked} -&gt; [B, C]。只有当 B 和 C 均从{credit_blocked} 中移除，A 才会解除阻塞，开始发送消息。 B 中用以记录接收到 A 发来消息数量的条目以{credit_to, To}为键，To 为进程 A 的 Pid。当 B 接收到超过 MoreCreditAfter 条消息后，会向 A 发送一条形如{bump_credit, {self{}, Quantity}}的消息，向 A 授予 credit, 这里的self()指的是 B 进程自身。值得注意的是，B 此时可能也被阻塞了，无法发送授予 credit 的消息，该消息暂时被放入键值对条目{credit_deferred} 中，直到条目{credit_blocked}内容为空，解除阻塞，再从{credit_deferred} 中取出被阻塞的消息重新发送。 对于 RabbitMQ 进程模型，如果 rabbit_reader 由于缺乏来自 rabbit_channel 授予的 credit 被阻塞，自然 client 也无法继续向下发布消息，由于每个 client 连接对应一对 rabbit_reader 与 rabbit_writer 进程，因此这种流控机制被认为是 Per-Connection 的。 References http://www.rabbitmq.com/configure.html http://www.rabbitmq.com/memory.html http://www.rabbitmq.com/production-checklist.html#resource-limits-ram http://alvaro-videla.com/2013/09/rabbitmq-internals-credit-flow-for-erlang-processes.html https://cloud.tencent.com/developer/article/1177577","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://yoursite.com/categories/RabbitMQ/"}],"tags":[{"name":"rabbitmq","slug":"rabbitmq","permalink":"http://yoursite.com/tags/rabbitmq/"}]},{"title":"kubernetes leader election 源码分析","slug":"k8s-leader-election","date":"2017-05-11T08:18:25.000Z","updated":"2018-11-24T14:51:22.717Z","comments":true,"path":"2017/05/11/k8s-leader-election/","link":"","permalink":"http://yoursite.com/2017/05/11/k8s-leader-election/","excerpt":"最近在写一个 kubernetes 的 addon，原理与 controller-manager 差不多。考虑到 addon 的高可用性需要将 kubernetes 源码包中的 leaderelection 包(k8s.io/kubernetes/pkg/client/leaderelection)移植到 client-go 中供调用，于是顺带分析了下这部分源码。 源码版本为 kubernetes release-1.5","text":"最近在写一个 kubernetes 的 addon，原理与 controller-manager 差不多。考虑到 addon 的高可用性需要将 kubernetes 源码包中的 leaderelection 包(k8s.io/kubernetes/pkg/client/leaderelection)移植到 client-go 中供调用，于是顺带分析了下这部分源码。 源码版本为 kubernetes release-1.5 controller-manager 与 scheduler 在启动的时候均可以添加--leader-elect参数以实现高可用性。原理大致与 HDFS 选主类似：抢占锁节点成为 leader，定时心跳保持 alive，leader 故障触发锁节点失效条件，non-leader 发起新一轮 leader election。 对于 HDFS 选主来说，锁节点指的是 Zookeeper 特定位置创建的一个临时节点ActiveStandbyElectorLock。任一 NameNode 率先创建该节点并写入自身信息即成为 Active NameNode（Zookeeper 的 API 与一致性保证了同一时间只会有一个 NameNode 创建该节点成功），剩余的 NameNode 成为 Standby NameNode，并在该临时节点上注册 Watcher。当 Active NameNode 因为某些原因跪了之后，考虑到临时 session 节点的特性，该节点会被删除，Standby NameNode 通过 Watcher 接收到节点删除事件，发起新一轮选举。 对于 k8s 的选主来说，锁节点指的是 kube-system 命名空间下的同名 endpoint。任一 goroutine 如果能成功在该 ep 的 annotation 中留下自身记号即成为 leader。成为 leader 后会定时续约，更新 annotation 中的相关过期时间戳。non-leader 们会定时去获取该 ep 的 annotation，若发现过期等情况则进行抢占。 leader 凭证主要有锁和租约两种。以我的理解，k8s 的 leader 凭证算是锁这一类的，强调主动性，主动去抢占写 ep 的 annotation。而 lease 租约更多是一种由某个机构主动颁发 lease，candidate 被动获得租约并成为 leader 的过程。 client 启动参数controller-manager 及 scheduler 中与 leader election 相关的启动参数有以下四个： leader-elect: 是否开启选举功能 leader-elect-lease-duration: 锁的失效时间，类似于 session-timeout leader-elect-renew-deadline: leader 的心跳间隔，必须小于等于 lease-duration leader-elect-retry-period: non-leader 每隔 retry-period 尝试获取锁 ResourceLock 锁结构12345678910111213// k8s.io/kubernetes/pkg/client/leaderelection/resourcelock/interface.goconst LeaderElectionRecordAnnotationKey = \"control-plane.alpha.kubernetes.io/leader\"type LeaderElectionRecord struct &#123; // leader 标识，通常为 hostname HolderIdentity string `json:\"holderIdentity\"` // 同启动参数 --leader-elect-lease-duration LeaseDurationSeconds int `json:\"leaseDurationSeconds\"` // Leader 第一次成功获得租约时的时间戳 AcquireTime unversioned.Time `json:\"acquireTime\"` // leader 定时 renew 的时间戳 RenewTime unversioned.Time `json:\"renewTime\"` LeaderTransitions int `json:\"leaderTransitions\"`&#125; leader 会在 kube-system namespace 下的同名 endpoint 的 annotations 中写入 key 值为LeaderElectionRecordAnnotationKey，value 为LeaderElectionRecord类型的记录，表明自己身份的同时会根据 –leader-elect-renew-deadline 参数定期去更新记录中的 RenewTime 字段（续约，合同年限为 LeaseDurationSeconds）。 lease 租约在分布式系统中的很多地方都有应用，如分布式锁、一致性问题等。租约机制可以一定程度上避免双主问题，确保同一时刻最多只有一个 leader。 1234567891011// k8s.io/kubernetes/pkg/client/leaderelection/resourcelock/interface.gotype Interface interface &#123; // 获取、创建、更新 annotations 中的选举记录 Get() (*LeaderElectionRecord, error) Create(ler LeaderElectionRecord) error Update(ler LeaderElectionRecord) error RecordEvent(string) Identity() string // endpoint namespace/name Describe() string&#125; k8s 中的选举锁需实现 resourcelock.Interface 接口，1.5 版本中实现的只有 endspointslock，不过貌似最近有 configmaplock 的 PR。endpointslock 实现该接口以实现 annotations 中 LeaderElectionRecord 的获取、创建、更新等功能，代码实现较为简单，不作详述。 LeaderElection 流程LeaderElector12345678910// k8s.io/kubernetes/pkg/client/leaderelection/leaderelection.gotype LeaderElector struct &#123; // 配置，基本同启动参数 config LeaderElectionConfig // 租约缓存 observedRecord rl.LeaderElectionRecord // 观察到租约缓存时的时间戳，用以判断租约是否到期 observedTime time.Time ...&#125; Leader Election Run12345678910111213141516// k8s.io/kubernetes/pkg/client/leaderelection/leaderelection.gofunc (le *LeaderElector) Run() &#123; defer func() &#123; runtime.HandleCrash() le.config.Callbacks.OnStoppedLeading() &#125;() // 获取租约，若获取不到则陷入定时循环获取租约，不执行下一条语句 le.acquire() stop := make(chan struct&#123;&#125;) // 成功获得租约，调用回调函数执行 leader goroutine 要干的活儿 go le.config.Callbacks.OnStartedLeading(stop) // 定时循环刷新租约，若刷新失败则说明被抢占了 leader，退出循环 le.renew() // 已不再是 leader，关闭 stop chan，停止干活儿 close(stop)&#125; acruire &amp;&amp; renew12345678910111213141516171819202122232425262728293031323334353637383940// k8s.io/kubernetes/pkg/client/leaderelection/leaderelection.go// 定时尝试获取租约，成功获得租约则立即退出函数func (le *LeaderElector) acquire() &#123; // 创建 stop chan，若成功获得租约，则关闭 stop chan，退出循环 stop := make(chan struct&#123;&#125;) wait.JitterUntil(func() &#123; succeeded := le.tryAcquireOrRenew() le.maybeReportTransition() desc := le.config.Lock.Describe() // 未能成功获得租约，进入下一定时循环 if !succeeded &#123; glog.V(4).Infof(\"failed to renew lease %v\", desc) return &#125; le.config.Lock.RecordEvent(\"became leader\") glog.Infof(\"sucessfully acquired lease %v\", desc) close(stop) &#125;, le.config.RetryPeriod, JitterFactor, true, stop)&#125; // 定时尝试续约，续约失败则立即退出函数func (le *LeaderElector) renew() &#123; stop := make(chan struct&#123;&#125;) wait.Until(func() &#123; err := wait.Poll(le.config.RetryPeriod, le.config.RenewDeadline, func() (bool, error) &#123; return le.tryAcquireOrRenew(), nil &#125;) le.maybeReportTransition() desc := le.config.Lock.Describe() // 续约成功，进入下一定时循环 if err == nil &#123; glog.V(4).Infof(\"succesfully renewed lease %v\", desc) return &#125; le.config.Lock.RecordEvent(\"stopped leading\") // 续约失败，关闭 stop chan，退出循环 glog.Infof(\"failed to renew lease %v\", desc) close(stop) &#125;, 0, stop)&#125; tryAcquireOrRenewtryAcquireOrRenew 函数尝试获取租约，如果获取不到或者得到的租约已过期则尝试抢占，否则 leader 不变。函数返回 True 说明本 goroutine 已成功抢占到锁，获得租约合同，成为 leader。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263// k8s.io/kubernetes/pkg/client/leaderelection/leaderelection.gofunc (le *LeaderElector) tryAcquireOrRenew() bool &#123; // 创建 leader election 租约 now := unversioned.Now() leaderElectionRecord := rl.LeaderElectionRecord&#123; HolderIdentity: le.config.Lock.Identity(), LeaseDurationSeconds: int(le.config.LeaseDuration / time.Second), RenewTime: now, AcquireTime: now, &#125; // 1. 从 endpointslock 上获取 leader election 租约（kube-system 下同名 endpoint 的 annotation) oldLeaderElectionRecord, err := le.config.Lock.Get() if err != nil &#123; // 发生 error，且 error 是除租约不存在以外的其它错误 if !errors.IsNotFound(err) &#123; glog.Errorf(\"error retrieving resource lock %v: %v\", le.config.Lock.Describe(), err) return false &#125; // 租约存在 // 于是将函数一开始创建的 leader election 租约放入同名 endpoint 的 annotation 中 if err = le.config.Lock.Create(leaderElectionRecord); err != nil &#123; // 创建失败，函数返回 false glog.Errorf(\"error initially creating leader election record: %v\", err) return false &#125; // 创建成功，成为 leader，函数返回 true le.observedRecord = leaderElectionRecord le.observedTime = time.Now() return true &#125; // 2. 更新本地缓存的租约，并更新观察时间戳，用来判断租约是否到期 if !reflect.DeepEqual(le.observedRecord, *oldLeaderElectionRecord) &#123; le.observedRecord = *oldLeaderElectionRecord le.observedTime = time.Now() &#125; // leader 的租约尚未到期，自己暂时不能抢占它，函数返回 false if le.observedTime.Add(le.config.LeaseDuration).After(now.Time) &amp;&amp; oldLeaderElectionRecord.HolderIdentity != le.config.Lock.Identity() &#123; glog.Infof(\"lock is held by %v and has not yet expired\", oldLeaderElectionRecord.HolderIdentity) return false &#125; // 3. 租约到期，而 leader 身份不变，因此当年获得租约的时间戳 AcquireTime 保持不变 if oldLeaderElectionRecord.HolderIdentity == le.config.Lock.Identity() &#123; leaderElectionRecord.AcquireTime = oldLeaderElectionRecord.AcquireTime &#125; else &#123; // 租约到期，leader 易主，transtions+1 说明 leader 更替了 leaderElectionRecord.LeaderTransitions = oldLeaderElectionRecord.LeaderTransitions + 1 &#125; // 尝试去更新租约记录 if err = le.config.Lock.Update(leaderElectionRecord); err != nil &#123; // 更新失败，函数返回 false glog.Errorf(\"Failed to update lock: %v\", err) return false &#125; // 更新成功，函数返回 true le.observedRecord = leaderElectionRecord le.observedTime = time.Now() return true&#125; kube-controller-manager 选举相关1234567891011121314151617181920212223242526272829// k8s.io/kubernetes/cmd/kube-controller-manager/app/controllermanager.go// 启动时在 kube-system namespace 下创建一个同名的 endpoint 作为 endpointslockrl := resourcelock.EndpointsLock&#123; EndpointsMeta: api.ObjectMeta&#123; Namespace: \"kube-system\", Name: \"kube-controller-manager\", &#125;, Client: leaderElectionClient, LockConfig: resourcelock.ResourceLockConfig&#123; // id = os.Hostname() Identity: id, EventRecorder: recorder, &#125;,&#125;// 开启选举流程leaderelection.RunOrDie(leaderelection.LeaderElectionConfig&#123; Lock: &amp;rl, LeaseDuration: s.LeaderElection.LeaseDuration.Duration, RenewDeadline: s.LeaderElection.RenewDeadline.Duration, RetryPeriod: s.LeaderElection.RetryPeriod.Duration, Callbacks: leaderelection.LeaderCallbacks&#123; // 成为 leader 后调用 run 函数启动各 controller goroutine OnStartedLeading: run, OnStoppedLeading: func() &#123; glog.Fatalf(\"leaderelection lost\") &#125;, &#125;,&#125;)panic(\"unreachable\") kube-scheduler 的相关源码基本类似，仅仅是 endpointslock 的名字改成 kube-scheduler，Callbacks 中的 run 函数改成了 scheduler 的启动函数 scheduler.Run()，启动 goroutine 执行调度流程。 从 leaderelection.go 源代码文件最上面的注释可以看到，这种选举的实现并不能保证不会出现脑裂的情况，即可能在某一时间点会有多个 client 同时扮演 leader 角色。client 是利用本地的时间戳去推测选举情况的，可以通过适当调整 RenewDeadline 与 LeaseDuration 或者架设 ntp 来解决。","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://yoursite.com/categories/Kubernetes/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/tags/docker/"},{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/tags/kubernetes/"}]},{"title":"使用 GitLab/Jenkins/Registry on Docker 搭建 CI/CD 环境","slug":"docker-gitlab-jekins","date":"2016-11-13T03:40:35.000Z","updated":"2018-11-24T15:45:58.938Z","comments":true,"path":"2016/11/13/docker-gitlab-jekins/","link":"","permalink":"http://yoursite.com/2016/11/13/docker-gitlab-jekins/","excerpt":"使用纯 Docker 搭建简易 CI/CD 环境，需要部署 GitLab/Jenkins/Regitry 组件。","text":"使用纯 Docker 搭建简易 CI/CD 环境，需要部署 GitLab/Jenkins/Regitry 组件。 使用 Docker 搭建环境GitLab参考 github 项目 docker-gitlab，可以通过 docker-compose 启动 GitLab，也可以分别启动 redis/postgresql/gitlab 容器并建立容器间的连接。 123wget https://raw.githubusercontent.com/sameersbn/docker-gitlab/master/docker-compose.yml# up 前按需更改下 volumes 配置docker-compose up 访问http://localhost:10080，使用默认用户名 root 和密码 5iveL!fe 即可以登录并使用 GitLab 创建 Group 和 Project 了。 GitLab 容器需开放两个端口，10080 端口负责 Web 界面访问，10022 端口负责 push 和 pull 代码。 当前基本是默认配置启动 GitLab，对于生产环境下的 GitLab，需要通过环境变量配置一系列参数，如 LDAP 或者 SSL 等。 Jenkins参考 github 项目 jenkinsci/docker，可以通过最新的 jenkins 镜像启动，也可以 clone 该项目并 docker build 该项目中的 Dockerfile。1234git clone https://github.com/jenkinsci/docker.gitcd dockerdocker build -t jenkins:jenkinsci .docker run --name myjenkins -p 8080:8080 -p 50000:50000 -v /var/jenkins_home jenkins:jenkinsci 考虑到在 Jenkins Container 中需要构建 docker 镜像，因此 docker build 前在 Dockerfile 中加入以下语句安装最新版 docker：1234RUN curl -O https://get.docker.com/builds/Linux/x86_64/docker-latest.tgz \\ &amp;&amp; tar -zxvf docker-latest.tgz \\ &amp;&amp; cp docker/docker /usr/local/bin/ \\ &amp;&amp; rm -rf docker docker-latest.tgz container 启动后终端日志会在 Please use the following password to proceed to installation 后打印出一段随机密码，需访问http://localhost:8080页面并输入该密码后 unlock jenkins，然后就可以正常使用了。 开放 8080 端口给 Web 界面，50000 端口给 slave agents（如果单机的话不需要） Registry1docker run -d --name myregistry -e STORAGE_PATH=/tmp/registry -v /tmp/registry -p 5000:5000 registry 这里直接利用 registry 的官方镜像启动。当然，对于生产环境下的 registry，还得加上 nginx 负载均衡/SSL/NFS 存储等配置，写入 config.yml 文件中，启动时加上 -e DOCKER_REGISTRY_CONFIG=config.yml。 DockerDocker 安装就没啥好说了，不过由于之后需要在 Jenkins Build 的过程中连接外部 Docker Daemon，需设置DOCKER_OPTS=&#39;-H unix:///var/run/docker.sock -H tcp://0.0.0.0.2375&#39;，并重新启动 docker。默认情况下 Docker 仅仅采用 sock。当然，也可以通过挂载 docker.sock 到 Jenkins 容器的方式来连接外部 Daemon。 Docker Container 作为 Jenkins Slave 1 Jenkins 安装 Docker Plugin 2 Jenkins-&gt;Manage Jenkins-&gt;Configure System 最下方会多出个 Cloud，点击 Add a new Cloud，填入 Docker URL、Images 和 Labels 等。可以点击 Test Connection 连接测试。 3 在创建的 Jenkins Item 中，General-&gt;勾选 Restrict where this project can be run，在 Label Expression 中填入先前创建的 Cloud 的 Labels 4 Jenkins Build 过程就会发生在该 Cloud 定义的容器中 https://hub.docker.com/u/jenkinsci/下有适合作为 jenkins slave 的image，如 jenkinsci/jnlp-slave 与 jenkinsci/ssh-slave，对应着不同的方式启动 slave 或执行命令。 利用 GitLab Webhook 触发 Jenkins Docker Build And Publish 1 GitLab 中创建一个包含 Dockerfile 的项目 2 Jenkins 安装两个插件：Gitlab Hook Plugin 和 CloudBees Docker Build And Publish plugin 3 Jenkins New Item 3.1 选择 Freestyle project 3.2 Source Code Management 中选择 Git，并填写 GitLab 中的项目仓库地址和选择 Branch 3.3 Build triggers 中选择 Trigger builds remotely，填入 Authentication Token 3.4 Build 中选择 Docker Build And Publish，填入 build 的镜像名称（前缀为 registry_host:port）、tag、docker host uri（tcp://docker_host:2375） 3.5 Save 4 GitLab-&gt;Project-&gt;Settings-&gt;Integrations 中添加 Webhook，http://jenkins_url/job/&lt;jenkins_item_name&gt;/build?token=&lt;Authentication Token&gt; GitLab 触发 Webhook 可能会发生 403 或者 anonymous login 的异常情况，需要在 Jenkins-&gt;Manage Jenkins-&gt;Configure Global Security 中，勾选 Allow anonymous read access 与去掉勾选 Prevent Cross Site Request Forgery exploits。 References 使用 Docker 搭建 GitLab 实践 使用 docker 构建 jenkins 镜像并运行容器 How To Setup Docker Containers As Build Slaves For Jenkins","categories":[{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/categories/Docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/tags/docker/"},{"name":"jenkins","slug":"jenkins","permalink":"http://yoursite.com/tags/jenkins/"},{"name":"devops","slug":"devops","permalink":"http://yoursite.com/tags/devops/"}]}]}