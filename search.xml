<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>Jaeger on Kubernetes 部署总结</title>
      <link href="/2018/09/19/jaeger-deploy/"/>
      <url>/2018/09/19/jaeger-deploy/</url>
      
        <content type="html"><![CDATA[<p>根据 Jaeger 1.6 官方的<a href="https://www.jaegertracing.io/docs/1.6/deployment/" target="_blank" rel="noopener">部署文档</a>，部署一个完整的 Jaeger 分布式链路追踪系统需要部署以下组件：</p><p><img src="https://s1.ax1x.com/2018/11/24/FkSD54.png" width="70%" height="70%" title="jaeger架构"></p><a id="more"></a><ul><li><strong>storage backend</strong>: 存储 trace 数据，支持 elasticsearch / cassandra / memory（仅支持 all-in-one 部署）以及 kafka（不完善，最新 1.6 版本仅支持写 trace，查询时需对接另一种 storage backend），通过环境变量 <code>SPAN_STORAGE_TYPE</code> 指定。</li><li><strong>jaeger-query</strong>: 从 storage 中查询 trace 及前端 ui 展示。</li><li><strong>jaeger-collector</strong>: 接收发送自 jaeger-agent 的 trace 数据（或者可直接发送 zipkin spans 至 collector），验证、索引、转换及存储 trace 数据。</li><li><strong>jaeger-agent</strong>: client 与 collector 中间的代理层，监听发送过来的 spans 数据并批量发送至 collector。</li></ul><blockquote><p>上述四种组件可直接通过 all-in-one 的方式部署（此时 storage backend 为 memory），由于存在 trace 数据丢失、组件单点故障等缺陷，仅建议开发环境下搭建使用。</p></blockquote><p>当前的 <a href="https://github.com/istio/istio/tree/master/install/kubernetes/helm/istio" target="_blank" rel="noopener">Istio Helm 套件</a>中包含了开启 jaeger 的<a href="https://github.com/istio/istio/blob/master/install/kubernetes/helm/istio/values.yaml#L498" target="_blank" rel="noopener">开关</a>（默认不开启），通过 helm 部署 istio 时加上 <code>--set tracing.enabled=true</code> 选项即可开启，在 istio-system namespace 下会创建一个名为 istio-tracing 的 deployment，以 all-in-one 方式部署。</p><p>推荐的部署方式是将各 jaeger 组件分散部署：</p><ul><li>query 与 collector 通过 deployment 部署（两者均为无状态服务）</li><li>agent 通过 daemonset 或者 sidecar container 部署</li><li>单独建立高可靠可用的 elasticsearch 或者 cassandra 集群</li></ul><h2 id="jaeger-agent-daemonset-vs-sidecar-pod"><a href="#jaeger-agent-daemonset-vs-sidecar-pod" class="headerlink" title="jaeger-agent daemonset vs sidecar pod"></a>jaeger-agent daemonset vs sidecar pod</h2><ul><li><strong>daemonset</strong>: 同一 node 上的所有 pod 将 trace 数据发往同一个 jaeger-agent，适用于单租户环境，同一 node 上的所有 pod 被同等对待。此部署方式的内存消耗较小，然而每一个 agent 可能需要照顾到几百个 pod。</li><li><strong>sidecar container</strong>: agent 作为 container 与 application pod 一同部署，运行在应用程序级别。不同的应用可发送 trace 数据至不同的 collector，适用于多租户环境。</li><li>前者适用于私有云，平台上运行的应用可信任；后者适用于公有云或者多租户需求，同时会带来一定的内存消耗（每个 agent 消耗大约 20MB 内存）。</li></ul><h2 id="elasticsearch-vs-cassandra"><a href="#elasticsearch-vs-cassandra" class="headerlink" title="elasticsearch vs cassandra"></a>elasticsearch vs cassandra</h2><ul><li>elasticsearch 查询性能优于 cassandra，cassandra 写性能相对更优</li><li>elasticsearch 的定位是 search engine，而不是 persistent store，偶尔会丢失部分 writes，对大量 trace 数据而言可接受（这点同 logs 与 metrics，接受部分数据丢失）</li></ul><p>此处选择 es 集群（对 es 运维比较熟悉），es 从 jaeger 0.6.0 时即开始支持，且不需要作手动初始化（cassandra 需手动创建 schema）。</p><h2 id="jaeger-kubernetes-vs-jaeger-helm-chart"><a href="#jaeger-kubernetes-vs-jaeger-helm-chart" class="headerlink" title="jaeger-kubernetes vs jaeger-helm-chart"></a>jaeger-kubernetes vs jaeger-helm-chart</h2><p><a href="https://github.com/helm/charts" target="_blank" rel="noopener">helm charts</a> 中包含了一个 <a href="https://github.com/helm/charts/tree/master/incubator/jaeger" target="_blank" rel="noopener">jaeger helm chart</a>，尚处于 incubator 阶段，到当前为止已有 4 个月未作任何更新，且版本较老，因此暂时放弃。</p><p><a href="https://github.com/jaegertracing/jaeger-kubernetes" target="_blank" rel="noopener">jaeger-kubernetes</a> 项目中包含了将 jaeger 组件分散部署的 <a href="https://github.com/jaegertracing/jaeger-kubernetes/blob/master/jaeger-production-template.yml" target="_blank" rel="noopener">yml 文件</a>，通过以下命令可创建一个分散部署的 jaeger：s<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 创建包含 es / jaeger 配置文件的 ConfigMap</span><br><span class="line"><span class="meta">$</span> kubectl create -f https://raw.githubusercontent.com/jaegertracing/jaeger-kubernetes/master/production-elasticsearch/configmap.yml </span><br><span class="line"><span class="meta">#</span> 创建单点 elasticsearch 集群</span><br><span class="line"><span class="meta">$</span> kubectl create -f https://raw.githubusercontent.com/jaegertracing/jaeger-kubernetes/master/production-elasticsearch/elasticsearch.yml </span><br><span class="line"><span class="meta">#</span> 创建组件分散部署的 jaeger（agent 通过 daemonset 部署）</span><br><span class="line"><span class="meta">$</span> kubectl create -f https://raw.githubusercontent.com/jaegertracing/jaeger-kubernetes/master/jaeger-production-template.yml</span><br></pre></td></tr></table></figure></p><p>通过上述方式部署的 es 集群采用的是 emptyDir volume，且以单点方式部署，同样不适用于生产环境。因此需要在使用上述 configmap.yml 与 jaeger-production-template.yml 部署 jaeger 之前，单独建立一个高可靠可用的 es 集群，通过 ceph/glusterfs 等存储保障 trace 数据不丢失。</p><blockquote><p>以上面的 yml 文件为基础，可以自行编写一套 helm chart，扩展各种 spec 配置，如 resource 和 affinity 等。</p></blockquote><h2 id="部署步骤"><a href="#部署步骤" class="headerlink" title="部署步骤"></a>部署步骤</h2><h3 id="创建-elasticsearch-集群"><a href="#创建-elasticsearch-集群" class="headerlink" title="创建 elasticsearch 集群"></a>创建 elasticsearch 集群</h3><p>假设当前 kubernetes 集群上包含 ceph storageclass，借助 helm charts 中的 <a href="https://github.com/helm/charts/tree/master/incubator/elasticsearch" target="_blank" rel="noopener">elasticsearch helm chart</a>：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> helm template elasticsearch --name jaeger-es --set cluster.name=tracing-es --set master.persistence.storageClass=ceph --set data.persistence.storageClass=ceph &gt; jaeger-es.yaml</span><br><span class="line"><span class="meta">$</span> kubectl create -f jaeger-es.yaml -n $&#123;namespace&#125;</span><br></pre></td></tr></table></figure></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">root@lin-k8s-dev1:~# kubectl -n $&#123;namespace&#125; get po | grep jaeger-es</span><br><span class="line">jaeger-es-elasticsearch-client-54666b4cf5-7lpbg   1/1       Running   2          1d</span><br><span class="line">jaeger-es-elasticsearch-client-54666b4cf5-scw79   1/1       Running   2          1d</span><br><span class="line">jaeger-es-elasticsearch-data-0                    1/1       Running   0          1d</span><br><span class="line">jaeger-es-elasticsearch-data-1                    1/1       Running   0          1d</span><br><span class="line">jaeger-es-elasticsearch-master-0                  1/1       Running   0          1d</span><br><span class="line">jaeger-es-elasticsearch-master-1                  1/1       Running   0          1d</span><br><span class="line">jaeger-es-elasticsearch-master-2                  1/1       Running   0          1d</span><br><span class="line">root@lin-k8s-dev1:~# kubectl -n $&#123;namespace&#125; get sts | grep jaeger-es</span><br><span class="line">jaeger-es-elasticsearch-data     2         2         1d</span><br><span class="line">jaeger-es-elasticsearch-master   3         3         1d</span><br><span class="line">root@lin-k8s-dev1:~# kubectl -n $&#123;namespace&#125; get deploy | grep jaeger-es</span><br><span class="line">jaeger-es-elasticsearch-client   2         2         2            2           1d</span><br><span class="line">root@lin-k8s-dev1:~# kubectl -n $&#123;namespace&#125; get cm | grep jaeger-es</span><br><span class="line">jaeger-es-elasticsearch                 4         1d</span><br><span class="line">root@lin-k8s-dev1:~# kubectl -n $&#123;namespace&#125; get svc | grep jaeger-es</span><br><span class="line">jaeger-es-elasticsearch-client      ClusterIP      10.109.173.100   &lt;none&gt;        9200/TCP                                                                                                                  1d</span><br><span class="line">jaeger-es-elasticsearch-discovery   ClusterIP      None             &lt;none&gt;        9300/TCP</span><br></pre></td></tr></table></figure><p>默认情况下会创建 es master statefulset (replica=3)、data statefulset (replica=2) 以及 client deployment (replica=2)。</p><blockquote><ol><li>es master 负责 es 集群的元数据维护及节点状态管理，data 负责真实 index 数据的查询及存储，client 负责分散负载。</li><li>可通过 service jaeger-es-elasticsearch-client（9200端口）与 es 集群进行交互：<code>curl http://jaeger-es-elasticsearch-client:9200/_cat/health?v</code></li><li>具体的 es 组件配置及资源配置需根据环境作针对调整。</li></ol></blockquote><h3 id="创建-jaeger"><a href="#创建-jaeger" class="headerlink" title="创建 jaeger"></a>创建 jaeger</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/jaegertracing/jaeger-kubernetes</span><br><span class="line">cd jaeger-kubernetes</span><br><span class="line"><span class="meta">#</span> 修改 production-elasticsearch/configmap.yml 中的两处 es.server-urls 为 http://jaeger-es-elasticsearch-client.$&#123;namespace&#125;:9200</span><br><span class="line">kubectl -n $&#123;namespace&#125; create -f production-elasticsearch/configmap.yml</span><br><span class="line"><span class="meta">#</span> 调整 jaeger-production-template.yml 中 query 及 collector deployments 的 replicas 及 resources 等字段</span><br><span class="line">kubectl -n $&#123;namespace&#125; create -f jaeger-production-template.yml</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">root@lin-k8s-dev1:~# kubectl -n $&#123;namespace&#125; get po | grep -v es | grep jaeger</span><br><span class="line">jaeger-agent-wtmn8                                1/1       Running   0          1d</span><br><span class="line">jaeger-collector-6b58d6c587-fs77b                 1/1       Running   0          1d</span><br><span class="line">jaeger-query-765c897dc4-p8nbh                     1/1       Running   0          1d</span><br><span class="line">root@lin-k8s-dev1:~# kubectl -n $&#123;namespace&#125; get deploy | grep -v es | grep jaeger</span><br><span class="line">jaeger-collector                 1         1         1            1           1d</span><br><span class="line">jaeger-query                     1         1         1            1           1d</span><br><span class="line">root@lin-k8s-dev1:~# kubectl -n $&#123;namespace&#125; get ds | grep -v es | grep jaeger</span><br><span class="line">jaeger-agent   1         1         1         1            1           &lt;none&gt;          1d</span><br><span class="line">root@lin-k8s-dev1:~# kubectl -n $&#123;namespace&#125; get cm | grep -v es | grep jaeger</span><br><span class="line">jaeger-configuration                    4         2d</span><br><span class="line">root@lin-k8s-dev1:~# kubectl -n $&#123;namespace&#125; get svc | grep -v es | grep 'jaeger\|zipkin'</span><br><span class="line">jaeger-collector                    ClusterIP      10.103.203.16    &lt;none&gt;        14267/TCP,14268/TCP,9411/TCP                                                                                              1d</span><br><span class="line">jaeger-query                        LoadBalancer   10.107.228.198   &lt;pending&gt;     80:31150/TCP                                                                                                              1d</span><br><span class="line">zipkin                              ClusterIP      10.109.123.192   &lt;none&gt;        9411/TCP                                                                                                                  1d</span><br></pre></td></tr></table></figure><blockquote><p>zipkin service 对应 jaeger-collector 的 9411 端口，zipkin spans 可直接通过该端口发送，无需经历 agent 代理。</p></blockquote><h3 id="与-istio-集成"><a href="#与-istio-集成" class="headerlink" title="与 istio 集成"></a>与 istio 集成</h3><p>应用程序 pod 中被注入的 sidecar container envoy 默认配置是将 spans 数据通过 <code>zipkin:9411</code> 发送给 jaeger-collector，而<code>jaeger-production-template.yml</code> 中已包含端口号是 9411 的 zipkin service，因此<strong>与 istio 集成的话暂时不需要做什么</strong>。</p><p>值得注意的是，如果 jaeger 仅仅是部署给 istio 使用，并且 HTTP 应用服务之间直接通过 headers forward (ZipkinB3HTTPHeader) 的方式传递 trace 信息，可以不用部署 jaeger-agent 组件，spans 数据会直接发送至 jaeger-collector。</p><p>为了 trace 每个 request 的信息，需要将 istio chart values.yml 中的 enableTracing 开启（默认是开启的），这样才会打开 <code>envoy.http_connection_manager</code> 的 tracing 功能。</p><blockquote><p>上述部署过程可直接形成一套 yaml，通过 apply -f 自动化部署，jaeger-query 与 jaeger-collector pod 初期由于连接不上 es 会频繁 CrashLoopBack 重启，待 es 集群启动完毕后，jaeger 组件也会进入 running 状态。</p></blockquote><h3 id="监控"><a href="#监控" class="headerlink" title="监控"></a>监控</h3><p>监控 elasticsearch 集群可直接采用开源项目 <a href="https://github.com/justwatchcom/elasticsearch_exporter" target="_blank" rel="noopener">elasticsearch_exporter</a>，helm charts 中包含了相应的部署 <a href="https://github.com/helm/charts/tree/master/stable/elasticsearch-exporter" target="_blank" rel="noopener">chart</a>。</p><p>jaeger 各组件的 metrics 可通过相应的 <a href="https://www.jaegertracing.io/docs/1.6/monitoring/#metrics" target="_blank" rel="noopener">HTTP 端口</a> 得到。根据 jaeger 的 metrics，可以对组件的一些配置进行调整（edit configmap）。以 jaeger-collector 暴露的 spans_dropped 指标为例，往往是由于内部的 queue overflow 导致，可以相应调大 collector 的参数 <code>collector.queue-size</code>（默认为 2000），并依次重启 jaeger-collector pod。</p>]]></content>
      
      
      <categories>
          
          <category> Tracing </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> jaeger </tag>
            
            <tag> tracing </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>RabbitMQ 之 Flow Control 机制</title>
      <link href="/2018/06/24/rabbitmq-flow-control/"/>
      <url>/2018/06/24/rabbitmq-flow-control/</url>
      
        <content type="html"><![CDATA[<p>最近尝试在将 RabbitMQ 搬上 Kubernetes 运行，因为有前车之鉴，担心 OOMKilled 的问题，特意调研一下 RabbitMQ 的流控机制。</p><p>当 RabbitMQ 消息消费速率小于发布速率，抑或是系统本身的资源就不太够时，为避免机器资源饱和，RabbitMQ 会阻断 publishers 进行流控。流控无需相关参数控制开启，这是 RabbitMQ 自身的默认行为。</p><a id="more"></a><h2 id="Memory-Based-Flow-Control"><a href="#Memory-Based-Flow-Control" class="headerlink" title="Memory-Based Flow Control"></a>Memory-Based Flow Control</h2><p>如同字面上的意思，对内存设置阈值，超出阈值后阻断发布连接，相关参数列举如下：</p><p><strong>total_memory_available_override_value</strong><br>系统内存值。默认情况下 RabbitMQ 启动时会推断该值的大小，在某些特殊环境下推断得到的值不大准确，比如在容器内部可能得到的是宿主机的内存值，需手动指定。</p><p><strong>vm_memory_high_watermark</strong><br>触发流控的内存阈值。默认为 0.4，意味着如果系统内存值为 1GB，则 RabbitMQ Server 在使用了 400MB 左右的内存时会阻断 pulish 连接。参数值的设置有相对与绝对两种：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vm_memory_high_watermark.relative = 0.6           </span><br><span class="line">vm_memory_high_watermark.absolute = 2GB</span><br></pre></td></tr></table></figure></p><p>设置完成后，可在日志中找到相关的内存限制信息：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># vm_memory_high_watermark.relative = 0.4, total_memory_available_override_value = 700MB</span><br><span class="line">Memory high watermark set to 280 MiB (293601280 bytes) of 700 MiB (734003200 bytes) total</span><br></pre></td></tr></table></figure></p><p>也可以在<code>rabbitmqctl status</code>的输出中找到：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">  &#123;vm_memory_high_watermark,0.4&#125;,</span><br><span class="line">  &#123;vm_memory_limit,293601280&#125;,</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p><ul><li>内存使用超过 40% 时，发出内存 alarm，阻塞连接；随着消息被消费或者被 page 到磁盘后，内存消耗降低，alarm 被撤销，publish 继续。</li><li><code>rabbitmqctl set_vm_memory_high_watermark 0</code> 可禁止一切 publish 操作。</li><li>默认设置为 0.4 是由于 Erlang 的垃圾回收机制在最坏情况下会导致内存消耗翻倍，即 80%。</li><li>参数值的设置并不能完全限制住 RabbitMQ 的内存阈值。</li></ul><p>官方文档对生产环境下参数值设置的建议：</p><ul><li>RabbitMQ 所在的机器最好时刻拥有 128MB 的空闲内存。</li><li>vm_memory_high_watermark 的推荐设置范围是 0.40 ~ 0.66。</li><li>不建议大于 0.7，以防大 paging 导致 OS 性能的急剧下降。</li></ul><p><strong>vm_memory_high_watermark_paging_ratio</strong><br>相对 vm_memory_high_watermark 触发消息 paging 至磁盘的内存比值。默认为 0.5，如果 vm_memory_high_watermark 设置为 0.4，系统内存值为 1GB，则 RabbitMQ 内存使用超过 0.5&times;0.4&times;1GB=200MB 时开始 paging。</p><p>当然，除了上述内存流控参数之外，还存在 <code>disk_free_limit</code> 等磁盘流控参数，具体说明可见官方文档。</p><h2 id="Per-Connection-Flow-Control"><a href="#Per-Connection-Flow-Control" class="headerlink" title="Per-Connection Flow Control"></a>Per-Connection Flow Control</h2><p>针对消息消费速率不及发布速率这种情况，应用实现方应该更加关注应用逻辑，而不是 RabbitMQ。然而 RabbitMQ 本身应当提供一种流控机制，防止由于出现类似情况而导致 OutOfDisk 或者 OOMKilled。除了之前提到的对内存和磁盘设置阈值之外，RabbitMQ 还默认提供了一种基于 credit flow 的流控机制，面向每一个连接进行流控，以确保 RabbitMQ 本身的稳定性。</p><p><img src="https://s1.ax1x.com/2018/11/24/FkSarV.png" alt="rabbitmq_connection_flow"></p><p>RabbitMQ 由 Erlang 实现，消息从发布到最终被消费，会在 RabbitMQ 内部的多个进程间进行转发，大致的转发路径如下图：</p><p><img src="https://s1.ax1x.com/2018/11/24/FkS0VU.png" width="80%" height="80%" title="rabbitmq进程模型"></p><p>rabbit_reader 接收 client 连接，解析 AMQP 帧；rabbit_channel 处理 AMQP 方法，路由消息；rabbit_amqqueue_process 作为队列进程，负责实现队列本身的所有逻辑，该进程数量一般与队列数目相同；rabbit_msg_store 负责消息持久化；rabbit_writer 向 client 返回数据。</p><p>作为 Erlang 进程，每个进程均拥有自己的 mailbox，进程间通过消息进行通信。mailbox 大小默认不作设置，如果某进程由于负载较高来不及处理消息，则 mailbox 开始堆积消息，最终可能会内存溢出进而进程崩溃。RabbitMQ 实现了一种基于 credit flow 的流控机制，当 mailbox 中的消息堆积到一定程度时会阻塞上游进程，不得接收新消息，一级级向上阻塞，最终会导致源头负责接收网络数据包的进程阻塞，进而阻塞发布。</p><p>每个消息处理进程拥有一个 credit 元组 <code>{InitialCredit, MoreCreditAfter}</code>，如果不调用 <code>credit_flow</code> API 调整值，默认为 <code>{200, 50}</code>。以进程 A 向进程 B 发送消息为例，A 的初始 credit 为 InitialCredit，A 每向 B 发送一条消息， A 的 credit 减 1，直到 credit 为 0 时 A 进程被阻塞；B 每收到 A 的 MoreCreditAfter 条消息后，会向 A 发送消息，授予 A MoreCreditAfter 个 credit。A 的 credit 大于 0 后恢复，继续向 B 发送消息。这些 credit 数量被记录在 Erlang 进程的进程字典（<a href="http://www.erlang.org/course/advanced#dict" target="_blank" rel="noopener">Process Dictionary</a>）中，相关字典键如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;credit_to, pid()&#125;</span><br><span class="line">&#123;credit_from, pid()&#125;</span><br><span class="line">&#123;credit_deferred&#125;</span><br><span class="line">&#123;credit_blocked&#125;</span><br></pre></td></tr></table></figure></p><p>A 中用以记录 B 授予 credit 数量的条目以<code>{credit_from, From}</code>为键，From 为进程 B 的 Pid，如果<code>{credit_from, From}</code>对应的值为 0，则 A 被阻塞，记录在以<code>{credit_blocked}</code>为键的条目中。当然，A 可能会向多个进程发送消息，比方说进程 C，这时候会同时存在以<code>{credit_from, B}</code>以及<code>{credit_from, C}</code>为键的条目；A 也可能会同时被进程 B 和 C 阻塞，阻塞进程的键值对条目大致如 <code>{credit_blocked} -&gt; [B, C]</code>。只有当 B 和 C 均从<code>{credit_blocked}</code> 中移除，A 才会解除阻塞，开始发送消息。</p><p>B 中用以记录接收到 A 发来消息数量的条目以<code>{credit_to, To}</code>为键，To 为进程 A 的 Pid。当 B 接收到超过 MoreCreditAfter 条消息后，会向 A 发送一条形如<code>{bump_credit, {self{}, Quantity}}</code>的消息，向 A 授予 credit, 这里的<code>self()</code>指的是 B 进程自身。值得注意的是，B 此时可能也被阻塞了，无法发送授予 credit 的消息，该消息暂时被放入键值对条目<code>{credit_deferred}</code> 中，直到条目<code>{credit_blocked}</code>内容为空，解除阻塞，再从<code>{credit_deferred}</code> 中取出被阻塞的消息重新发送。</p><p>对于 RabbitMQ 进程模型，如果 rabbit_reader 由于缺乏来自 rabbit_channel 授予的 credit 被阻塞，自然 client 也无法继续向下发布消息，由于每个 client 连接对应一对 rabbit_reader 与 rabbit_writer 进程，因此这种流控机制被认为是 Per-Connection 的。</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="http://www.rabbitmq.com/configure.html" target="_blank" rel="noopener">http://www.rabbitmq.com/configure.html</a></li><li><a href="http://www.rabbitmq.com/memory.html" target="_blank" rel="noopener">http://www.rabbitmq.com/memory.html</a></li><li><a href="http://www.rabbitmq.com/production-checklist.html#resource-limits-ram" target="_blank" rel="noopener">http://www.rabbitmq.com/production-checklist.html#resource-limits-ram</a></li><li><a href="http://alvaro-videla.com/2013/09/rabbitmq-internals-credit-flow-for-erlang-processes.html" target="_blank" rel="noopener">http://alvaro-videla.com/2013/09/rabbitmq-internals-credit-flow-for-erlang-processes.html</a></li><li><a href="https://cloud.tencent.com/developer/article/1177577" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1177577</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> RabbitMQ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rabbitmq </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>kubernetes leader election 源码分析</title>
      <link href="/2017/05/11/k8s-leader-election/"/>
      <url>/2017/05/11/k8s-leader-election/</url>
      
        <content type="html"><![CDATA[<p>最近在写一个 kubernetes 的 addon，原理与 controller-manager 差不多。考虑到 addon 的高可用性需要将 kubernetes 源码包中的 leaderelection 包(k8s.io/kubernetes/pkg/client/leaderelection)移植到 client-go 中供调用，于是顺带分析了下这部分源码。</p><blockquote><p>源码版本为 kubernetes release-1.5</p></blockquote><a id="more"></a><p>controller-manager 与 scheduler 在启动的时候均可以添加<code>--leader-elect</code>参数以实现高可用性。原理大致与 HDFS 选主类似：抢占锁节点成为 leader，定时心跳保持 alive，leader 故障触发锁节点失效条件，non-leader 发起新一轮 leader election。</p><p>对于 HDFS 选主来说，锁节点指的是 Zookeeper 特定位置创建的一个临时节点<code>ActiveStandbyElectorLock</code>。任一 NameNode 率先创建该节点并写入自身信息即成为 Active NameNode（Zookeeper 的 API 与一致性保证了同一时间只会有一个 NameNode 创建该节点成功），剩余的 NameNode 成为 Standby NameNode，并在该临时节点上注册 Watcher。当 Active NameNode 因为某些原因跪了之后，考虑到临时 session 节点的特性，该节点会被删除，Standby NameNode 通过 Watcher 接收到节点删除事件，发起新一轮选举。</p><p>对于 k8s 的选主来说，锁节点指的是 kube-system 命名空间下的同名 endpoint。任一 goroutine 如果能成功在该 ep 的 annotation 中留下自身记号即成为 leader。成为 leader 后会定时续约，更新 annotation 中的相关过期时间戳。non-leader 们会定时去获取该 ep 的 annotation，若发现过期等情况则进行抢占。</p><blockquote><p>leader 凭证主要有锁和租约两种。以我的理解，k8s 的 leader 凭证算是锁这一类的，强调主动性，主动去抢占写 ep 的 annotation。而 lease 租约更多是一种由某个机构主动颁发 lease，candidate 被动获得租约并成为 leader 的过程。</p></blockquote><h2 id="client-启动参数"><a href="#client-启动参数" class="headerlink" title="client 启动参数"></a>client 启动参数</h2><p>controller-manager 及 scheduler 中与 leader election 相关的启动参数有以下四个：</p><ul><li>leader-elect: 是否开启选举功能</li><li>leader-elect-lease-duration: 锁的失效时间，类似于 session-timeout</li><li>leader-elect-renew-deadline: leader 的心跳间隔，必须小于等于 lease-duration</li><li>leader-elect-retry-period: non-leader 每隔 retry-period 尝试获取锁</li></ul><h2 id="ResourceLock-锁结构"><a href="#ResourceLock-锁结构" class="headerlink" title="ResourceLock 锁结构"></a>ResourceLock 锁结构</h2><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// k8s.io/kubernetes/pkg/client/leaderelection/resourcelock/interface.go</span></span><br><span class="line"><span class="keyword">const</span> LeaderElectionRecordAnnotationKey = <span class="string">"control-plane.alpha.kubernetes.io/leader"</span></span><br><span class="line"><span class="keyword">type</span> LeaderElectionRecord <span class="keyword">struct</span> &#123;</span><br><span class="line">  <span class="comment">// leader 标识，通常为 hostname</span></span><br><span class="line">  HolderIdentity       <span class="keyword">string</span>           <span class="string">`json:"holderIdentity"`</span></span><br><span class="line">  <span class="comment">// 同启动参数 --leader-elect-lease-duration</span></span><br><span class="line">  LeaseDurationSeconds <span class="keyword">int</span>              <span class="string">`json:"leaseDurationSeconds"`</span></span><br><span class="line">  <span class="comment">// Leader 第一次成功获得租约时的时间戳</span></span><br><span class="line">  AcquireTime          unversioned.Time <span class="string">`json:"acquireTime"`</span></span><br><span class="line">  <span class="comment">// leader 定时 renew 的时间戳</span></span><br><span class="line">  RenewTime            unversioned.Time <span class="string">`json:"renewTime"`</span></span><br><span class="line">  LeaderTransitions    <span class="keyword">int</span>              <span class="string">`json:"leaderTransitions"`</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>leader 会在 kube-system namespace 下的同名 endpoint 的 annotations 中写入 key 值为<code>LeaderElectionRecordAnnotationKey</code>，value 为<code>LeaderElectionRecord</code>类型的记录，表明自己身份的同时会根据 –leader-elect-renew-deadline 参数定期去更新记录中的 RenewTime 字段（续约，合同年限为 LeaseDurationSeconds）。</p><p>lease 租约在分布式系统中的很多地方都有应用，如分布式锁、一致性问题等。租约机制可以一定程度上避免双主问题，确保同一时刻最多只有一个 leader。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// k8s.io/kubernetes/pkg/client/leaderelection/resourcelock/interface.go</span></span><br><span class="line"><span class="keyword">type</span> Interface <span class="keyword">interface</span> &#123;</span><br><span class="line">  <span class="comment">// 获取、创建、更新 annotations 中的选举记录</span></span><br><span class="line">  Get() (*LeaderElectionRecord, error)</span><br><span class="line">  Create(ler LeaderElectionRecord) error</span><br><span class="line">  Update(ler LeaderElectionRecord) error</span><br><span class="line">  RecordEvent(<span class="keyword">string</span>)</span><br><span class="line">  Identity() <span class="keyword">string</span></span><br><span class="line">  <span class="comment">// endpoint namespace/name</span></span><br><span class="line">  Describe() <span class="keyword">string</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>k8s 中的选举锁需实现 resourcelock.Interface 接口，1.5 版本中实现的只有 endspointslock，不过貌似最近有 configmaplock 的 PR。endpointslock 实现该接口以实现 annotations 中 LeaderElectionRecord 的获取、创建、更新等功能，代码实现较为简单，不作详述。</p><h2 id="LeaderElection-流程"><a href="#LeaderElection-流程" class="headerlink" title="LeaderElection 流程"></a>LeaderElection 流程</h2><h3 id="LeaderElector"><a href="#LeaderElector" class="headerlink" title="LeaderElector"></a>LeaderElector</h3><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// k8s.io/kubernetes/pkg/client/leaderelection/leaderelection.go</span></span><br><span class="line"><span class="keyword">type</span> LeaderElector <span class="keyword">struct</span> &#123;</span><br><span class="line">  <span class="comment">// 配置，基本同启动参数</span></span><br><span class="line">  config LeaderElectionConfig</span><br><span class="line">  <span class="comment">// 租约缓存</span></span><br><span class="line">  observedRecord rl.LeaderElectionRecord</span><br><span class="line">  <span class="comment">// 观察到租约缓存时的时间戳，用以判断租约是否到期</span></span><br><span class="line">  observedTime   time.Time</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Leader-Election-Run"><a href="#Leader-Election-Run" class="headerlink" title="Leader Election Run"></a>Leader Election Run</h3><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// k8s.io/kubernetes/pkg/client/leaderelection/leaderelection.go</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(le *LeaderElector)</span> <span class="title">Run</span><span class="params">()</span></span> &#123;</span><br><span class="line">  <span class="keyword">defer</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">    runtime.HandleCrash()</span><br><span class="line">    le.config.Callbacks.OnStoppedLeading()</span><br><span class="line">  &#125;()</span><br><span class="line">  <span class="comment">// 获取租约，若获取不到则陷入定时循环获取租约，不执行下一条语句</span></span><br><span class="line">  le.acquire()</span><br><span class="line">  stop := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">struct</span>&#123;&#125;)</span><br><span class="line">  <span class="comment">// 成功获得租约，调用回调函数执行 leader goroutine 要干的活儿</span></span><br><span class="line">  <span class="keyword">go</span> le.config.Callbacks.OnStartedLeading(stop)</span><br><span class="line">  <span class="comment">// 定时循环刷新租约，若刷新失败则说明被抢占了 leader，退出循环</span></span><br><span class="line">  le.renew()</span><br><span class="line">  <span class="comment">// 已不再是 leader，关闭 stop chan，停止干活儿</span></span><br><span class="line">  <span class="built_in">close</span>(stop)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="acruire-amp-amp-renew"><a href="#acruire-amp-amp-renew" class="headerlink" title="acruire &amp;&amp; renew"></a>acruire &amp;&amp; renew</h3><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// k8s.io/kubernetes/pkg/client/leaderelection/leaderelection.go</span></span><br><span class="line"><span class="comment">// 定时尝试获取租约，成功获得租约则立即退出函数</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(le *LeaderElector)</span> <span class="title">acquire</span><span class="params">()</span></span> &#123;</span><br><span class="line">   <span class="comment">// 创建 stop chan，若成功获得租约，则关闭 stop chan，退出循环</span></span><br><span class="line">  stop := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">struct</span>&#123;&#125;)</span><br><span class="line">  wait.JitterUntil(<span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">    succeeded := le.tryAcquireOrRenew()</span><br><span class="line">    le.maybeReportTransition()</span><br><span class="line">    desc := le.config.Lock.Describe()</span><br><span class="line">    <span class="comment">// 未能成功获得租约，进入下一定时循环</span></span><br><span class="line">    <span class="keyword">if</span> !succeeded &#123;</span><br><span class="line">      glog.V(<span class="number">4</span>).Infof(<span class="string">"failed to renew lease %v"</span>, desc)</span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line">    le.config.Lock.RecordEvent(<span class="string">"became leader"</span>)</span><br><span class="line">    glog.Infof(<span class="string">"sucessfully acquired lease %v"</span>, desc)</span><br><span class="line">    <span class="built_in">close</span>(stop)</span><br><span class="line">  &#125;, le.config.RetryPeriod, JitterFactor, <span class="literal">true</span>, stop)</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="comment">// 定时尝试续约，续约失败则立即退出函数</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(le *LeaderElector)</span> <span class="title">renew</span><span class="params">()</span></span> &#123;</span><br><span class="line">  stop := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">struct</span>&#123;&#125;)</span><br><span class="line">  wait.Until(<span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">    err := wait.Poll(le.config.RetryPeriod, le.config.RenewDeadline, <span class="function"><span class="keyword">func</span><span class="params">()</span> <span class="params">(<span class="keyword">bool</span>, error)</span></span> &#123;</span><br><span class="line">      <span class="keyword">return</span> le.tryAcquireOrRenew(), <span class="literal">nil</span></span><br><span class="line">    &#125;)</span><br><span class="line">    le.maybeReportTransition()</span><br><span class="line">    desc := le.config.Lock.Describe()</span><br><span class="line">    <span class="comment">// 续约成功，进入下一定时循环</span></span><br><span class="line">    <span class="keyword">if</span> err == <span class="literal">nil</span> &#123;</span><br><span class="line">      glog.V(<span class="number">4</span>).Infof(<span class="string">"succesfully renewed lease %v"</span>, desc)</span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line">    le.config.Lock.RecordEvent(<span class="string">"stopped leading"</span>)</span><br><span class="line">    <span class="comment">// 续约失败，关闭 stop chan，退出循环</span></span><br><span class="line">    glog.Infof(<span class="string">"failed to renew lease %v"</span>, desc)</span><br><span class="line">    <span class="built_in">close</span>(stop)</span><br><span class="line">  &#125;, <span class="number">0</span>, stop)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="tryAcquireOrRenew"><a href="#tryAcquireOrRenew" class="headerlink" title="tryAcquireOrRenew"></a>tryAcquireOrRenew</h3><p>tryAcquireOrRenew 函数尝试获取租约，如果获取不到或者得到的租约已过期则尝试抢占，否则 leader 不变。函数返回 True 说明本 goroutine 已成功抢占到锁，获得租约合同，成为 leader。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// k8s.io/kubernetes/pkg/client/leaderelection/leaderelection.go</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(le *LeaderElector)</span> <span class="title">tryAcquireOrRenew</span><span class="params">()</span> <span class="title">bool</span></span> &#123;</span><br><span class="line">  <span class="comment">// 创建 leader election 租约</span></span><br><span class="line">  now := unversioned.Now()</span><br><span class="line">  leaderElectionRecord := rl.LeaderElectionRecord&#123;</span><br><span class="line">    HolderIdentity:       le.config.Lock.Identity(),</span><br><span class="line">    LeaseDurationSeconds: <span class="keyword">int</span>(le.config.LeaseDuration / time.Second),</span><br><span class="line">    RenewTime:            now,</span><br><span class="line">    AcquireTime:          now,</span><br><span class="line">  &#125;</span><br><span class="line"> </span><br><span class="line">  <span class="comment">// 1. 从 endpointslock 上获取 leader election 租约（kube-system 下同名 endpoint 的 annotation)</span></span><br><span class="line">  oldLeaderElectionRecord, err := le.config.Lock.Get()</span><br><span class="line">  <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">    <span class="comment">// 发生 error，且 error 是除租约不存在以外的其它错误</span></span><br><span class="line">    <span class="keyword">if</span> !errors.IsNotFound(err) &#123;</span><br><span class="line">      glog.Errorf(<span class="string">"error retrieving resource lock %v: %v"</span>, le.config.Lock.Describe(), err)</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 租约存在</span></span><br><span class="line">    <span class="comment">// 于是将函数一开始创建的 leader election 租约放入同名 endpoint 的 annotation 中</span></span><br><span class="line">    <span class="keyword">if</span> err = le.config.Lock.Create(leaderElectionRecord); err != <span class="literal">nil</span> &#123;</span><br><span class="line">       <span class="comment">// 创建失败，函数返回 false</span></span><br><span class="line">      glog.Errorf(<span class="string">"error initially creating leader election record: %v"</span>, err)</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 创建成功，成为 leader，函数返回 true</span></span><br><span class="line">    le.observedRecord = leaderElectionRecord</span><br><span class="line">    le.observedTime = time.Now()</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span></span><br><span class="line">  &#125;</span><br><span class="line"> </span><br><span class="line">  <span class="comment">// 2. 更新本地缓存的租约，并更新观察时间戳，用来判断租约是否到期</span></span><br><span class="line">  <span class="keyword">if</span> !reflect.DeepEqual(le.observedRecord, *oldLeaderElectionRecord) &#123;</span><br><span class="line">    le.observedRecord = *oldLeaderElectionRecord</span><br><span class="line">    le.observedTime = time.Now()</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// leader 的租约尚未到期，自己暂时不能抢占它，函数返回 false</span></span><br><span class="line">  <span class="keyword">if</span> le.observedTime.Add(le.config.LeaseDuration).After(now.Time) &amp;&amp;</span><br><span class="line">    oldLeaderElectionRecord.HolderIdentity != le.config.Lock.Identity() &#123;</span><br><span class="line">    glog.Infof(<span class="string">"lock is held by %v and has not yet expired"</span>, oldLeaderElectionRecord.HolderIdentity)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line">  &#125;</span><br><span class="line"> </span><br><span class="line">  <span class="comment">// 3. 租约到期，而 leader 身份不变，因此当年获得租约的时间戳 AcquireTime 保持不变</span></span><br><span class="line">  <span class="keyword">if</span> oldLeaderElectionRecord.HolderIdentity == le.config.Lock.Identity() &#123;</span><br><span class="line">    leaderElectionRecord.AcquireTime = oldLeaderElectionRecord.AcquireTime</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// 租约到期，leader 易主，transtions+1 说明 leader 更替了</span></span><br><span class="line">    leaderElectionRecord.LeaderTransitions = oldLeaderElectionRecord.LeaderTransitions + <span class="number">1</span></span><br><span class="line">  &#125;</span><br><span class="line"> </span><br><span class="line">  <span class="comment">// 尝试去更新租约记录</span></span><br><span class="line">  <span class="keyword">if</span> err = le.config.Lock.Update(leaderElectionRecord); err != <span class="literal">nil</span> &#123;</span><br><span class="line">     <span class="comment">// 更新失败，函数返回 false</span></span><br><span class="line">    glog.Errorf(<span class="string">"Failed to update lock: %v"</span>, err)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 更新成功，函数返回 true</span></span><br><span class="line">  le.observedRecord = leaderElectionRecord</span><br><span class="line">  le.observedTime = time.Now()</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">true</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="kube-controller-manager-选举相关"><a href="#kube-controller-manager-选举相关" class="headerlink" title="kube-controller-manager 选举相关"></a>kube-controller-manager 选举相关</h2><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// k8s.io/kubernetes/cmd/kube-controller-manager/app/controllermanager.go</span></span><br><span class="line"><span class="comment">// 启动时在 kube-system namespace 下创建一个同名的 endpoint 作为 endpointslock</span></span><br><span class="line">rl := resourcelock.EndpointsLock&#123;</span><br><span class="line">  EndpointsMeta: api.ObjectMeta&#123;</span><br><span class="line">    Namespace: <span class="string">"kube-system"</span>,</span><br><span class="line">    Name:      <span class="string">"kube-controller-manager"</span>,</span><br><span class="line">  &#125;,</span><br><span class="line">  Client: leaderElectionClient,</span><br><span class="line">  LockConfig: resourcelock.ResourceLockConfig&#123;</span><br><span class="line">    <span class="comment">// id = os.Hostname()</span></span><br><span class="line">    Identity:      id,</span><br><span class="line">    EventRecorder: recorder,</span><br><span class="line">  &#125;,</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 开启选举流程</span></span><br><span class="line">leaderelection.RunOrDie(leaderelection.LeaderElectionConfig&#123;</span><br><span class="line">  Lock:          &amp;rl,</span><br><span class="line">  LeaseDuration: s.LeaderElection.LeaseDuration.Duration,</span><br><span class="line">  RenewDeadline: s.LeaderElection.RenewDeadline.Duration,</span><br><span class="line">  RetryPeriod:   s.LeaderElection.RetryPeriod.Duration,</span><br><span class="line">  Callbacks: leaderelection.LeaderCallbacks&#123;</span><br><span class="line">    <span class="comment">// 成为 leader 后调用 run 函数启动各 controller goroutine</span></span><br><span class="line">    OnStartedLeading: run,</span><br><span class="line">    OnStoppedLeading: <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">      glog.Fatalf(<span class="string">"leaderelection lost"</span>)</span><br><span class="line">    &#125;,</span><br><span class="line">  &#125;,</span><br><span class="line">&#125;)</span><br><span class="line"><span class="built_in">panic</span>(<span class="string">"unreachable"</span>)</span><br></pre></td></tr></table></figure><p>kube-scheduler 的相关源码基本类似，仅仅是 endpointslock 的名字改成 kube-scheduler，Callbacks 中的 run 函数改成了 scheduler 的启动函数 scheduler.Run()，启动 goroutine 执行调度流程。</p><blockquote><p>从 leaderelection.go 源代码文件最上面的注释可以看到，这种选举的实现并不能保证不会出现脑裂的情况，即可能在某一时间点会有多个 client 同时扮演 leader 角色。client 是利用本地的时间戳去推测选举情况的，可以通过适当调整 RenewDeadline 与 LeaseDuration 或者架设 ntp 来解决。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>使用 GitLab/Jenkins/Registry on Docker 搭建 CI/CD 环境</title>
      <link href="/2016/11/13/docker-gitlab-jekins/"/>
      <url>/2016/11/13/docker-gitlab-jekins/</url>
      
        <content type="html"><![CDATA[<p>使用纯 Docker 搭建简易 CI/CD 环境，需要部署 GitLab/Jenkins/Regitry 组件。</p><a id="more"></a><h2 id="使用-Docker-搭建环境"><a href="#使用-Docker-搭建环境" class="headerlink" title="使用 Docker 搭建环境"></a>使用 Docker 搭建环境</h2><h3 id="GitLab"><a href="#GitLab" class="headerlink" title="GitLab"></a>GitLab</h3><p>参考 github 项目 <a href="https://github.com/sameersbn/docker-gitlab" target="_blank" rel="noopener">docker-gitlab</a>，可以通过 docker-compose 启动 GitLab，也可以分别启动 redis/postgresql/gitlab 容器并建立容器间的连接。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wget https://raw.githubusercontent.com/sameersbn/docker-gitlab/master/docker-compose.yml</span><br><span class="line"><span class="meta">#</span> up 前按需更改下 volumes 配置</span><br><span class="line">docker-compose up</span><br></pre></td></tr></table></figure><p>访问<code>http://localhost:10080</code>，使用默认用户名 root 和密码 5iveL!fe 即可以登录并使用 GitLab 创建 Group 和 Project 了。</p><blockquote><ol><li>GitLab 容器需开放两个端口，10080 端口负责 Web 界面访问，10022 端口负责 push 和 pull 代码。 </li><li>当前基本是默认配置启动 GitLab，对于生产环境下的 GitLab，需要通过<a href="https://github.com/sameersbn/docker-gitlab#available-configuration-parameters" target="_blank" rel="noopener">环境变量</a>配置一系列参数，如 LDAP 或者 SSL 等。</li></ol></blockquote><h3 id="Jenkins"><a href="#Jenkins" class="headerlink" title="Jenkins"></a>Jenkins</h3><p>参考 github 项目 <a href="https://github.com/jenkinsci/docker" target="_blank" rel="noopener">jenkinsci/docker</a>，可以通过最新的 jenkins 镜像启动，也可以 clone 该项目并 docker build 该项目中的 Dockerfile。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/jenkinsci/docker.git</span><br><span class="line">cd docker</span><br><span class="line">docker build -t jenkins:jenkinsci .</span><br><span class="line">docker run --name myjenkins -p 8080:8080 -p 50000:50000 -v /var/jenkins_home jenkins:jenkinsci</span><br></pre></td></tr></table></figure></p><p>考虑到在 Jenkins Container 中需要构建 docker 镜像，因此 docker build 前在 Dockerfile 中加入以下语句安装最新版 docker：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">RUN curl -O https://get.docker.com/builds/Linux/x86_64/docker-latest.tgz \ </span><br><span class="line">    &amp;&amp; tar -zxvf docker-latest.tgz \</span><br><span class="line">    &amp;&amp; cp docker/docker /usr/local/bin/ \</span><br><span class="line">    &amp;&amp; rm -rf docker docker-latest.tgz</span><br></pre></td></tr></table></figure></p><p>container 启动后终端日志会在 Please use the following password to proceed to installation 后打印出一段随机密码，需访问<code>http://localhost:8080</code>页面并输入该密码后 unlock jenkins，然后就可以正常使用了。</p><blockquote><ol><li>开放 8080 端口给 Web 界面，50000 端口给 slave agents（如果单机的话不需要）</li></ol></blockquote><h3 id="Registry"><a href="#Registry" class="headerlink" title="Registry"></a>Registry</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name myregistry  -e STORAGE_PATH=/tmp/registry -v /tmp/registry -p 5000:5000 registry</span><br></pre></td></tr></table></figure><p>这里直接利用 registry 的官方镜像启动。当然，对于生产环境下的 registry，还得加上 nginx 负载均衡/SSL/NFS 存储等配置，写入 config.yml 文件中，启动时加上 -e DOCKER_REGISTRY_CONFIG=config.yml。</p><h3 id="Docker"><a href="#Docker" class="headerlink" title="Docker"></a>Docker</h3><p>Docker 安装就没啥好说了，不过由于之后需要在 Jenkins Build 的过程中连接外部 Docker Daemon，需设置<code>DOCKER_OPTS=&#39;-H unix:///var/run/docker.sock -H tcp://0.0.0.0.2375&#39;</code>，并重新启动 docker。默认情况下 Docker 仅仅采用 sock。当然，也可以通过挂载 docker.sock 到 Jenkins 容器的方式来连接外部 Daemon。</p><h2 id="Docker-Container-作为-Jenkins-Slave"><a href="#Docker-Container-作为-Jenkins-Slave" class="headerlink" title="Docker Container 作为 Jenkins Slave"></a>Docker Container 作为 Jenkins Slave</h2><ul><li>1 Jenkins 安装 Docker Plugin</li><li>2 Jenkins-&gt;Manage Jenkins-&gt;Configure System 最下方会多出个 Cloud，点击 Add a new Cloud，填入 Docker URL、Images 和 <strong>Labels</strong> 等。可以点击 Test Connection 连接测试。<br><img src="https://s1.ax1x.com/2018/11/24/FkSdbT.png" alt="jenkins-cloud-config"></li><li>3 在创建的 Jenkins Item 中，General-&gt;勾选 Restrict where this project can be run，在 Label Expression 中填入先前创建的 Cloud 的 Labels</li><li>4 Jenkins Build 过程就会发生在该 Cloud 定义的容器中</li></ul><blockquote><p><code>https://hub.docker.com/u/jenkinsci/</code>下有适合作为 jenkins slave 的image，如 jenkinsci/jnlp-slave 与 jenkinsci/ssh-slave，对应着不同的方式启动 slave 或执行命令。</p></blockquote><h2 id="利用-GitLab-Webhook-触发-Jenkins-Docker-Build-And-Publish"><a href="#利用-GitLab-Webhook-触发-Jenkins-Docker-Build-And-Publish" class="headerlink" title="利用 GitLab Webhook 触发 Jenkins Docker Build And Publish"></a>利用 GitLab Webhook 触发 Jenkins Docker Build And Publish</h2><ul><li>1 GitLab 中创建一个包含 Dockerfile 的项目</li><li>2 Jenkins 安装两个插件：Gitlab Hook Plugin 和 CloudBees Docker Build And Publish plugin</li><li>3 Jenkins New Item<ul><li>3.1 选择 Freestyle project</li><li>3.2 Source Code Management 中选择 Git，并填写 GitLab 中的项目仓库地址和选择 Branch</li><li>3.3 Build triggers 中选择 Trigger builds remotely，填入 Authentication Token</li><li>3.4 Build 中选择 Docker Build And Publish，填入 build 的镜像名称（前缀为 registry_host:port）、tag、docker host uri（<code>tcp://docker_host:2375</code>）</li><li>3.5 Save</li></ul></li><li>4 GitLab-&gt;Project-&gt;Settings-&gt;Integrations 中添加 Webhook，<code>http://jenkins_url/job/&lt;jenkins_item_name&gt;/build?token=&lt;Authentication Token&gt;</code></li></ul><blockquote><p>GitLab 触发 Webhook 可能会发生 403 或者 anonymous login 的异常情况，需要在 Jenkins-&gt;Manage Jenkins-&gt;Configure Global Security 中，勾选 Allow anonymous read access 与去掉勾选 Prevent Cross Site Request Forgery exploits。</p></blockquote><p><img src="https://s1.ax1x.com/2018/11/24/FkSBaF.jpg" alt="jenkins-docker-build"></p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="http://www.open-open.com/lib/view/open1438009785316.html" target="_blank" rel="noopener">使用 Docker 搭建 GitLab 实践</a></li><li><a href="https://my.oschina.net/donhui/blog/470372" target="_blank" rel="noopener">使用 docker 构建 jenkins 镜像并运行容器</a></li><li><a href="https://devopscube.com/docker-containers-as-build-slaves-jenkins/" target="_blank" rel="noopener">How To Setup Docker Containers As Build Slaves For Jenkins</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
            <tag> jenkins </tag>
            
            <tag> devops </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
